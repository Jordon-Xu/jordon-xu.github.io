<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Automatic Driving Experiment Platform Based on Machine Vision | Jordon-Xu's Blog</title><meta name="author" content="Jordon_Xu"><meta name="copyright" content="Jordon_Xu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AbstractThe iteration of the automobile industry constantly promotes the rapid development of human economy and society. However, traffic jams and traffic accidents and other problems often cause trou">
<meta property="og:type" content="article">
<meta property="og:title" content="Automatic Driving Experiment Platform Based on Machine Vision">
<meta property="og:url" content="http://example.com/2021/10/02/Automatic-Driving-Experiment-Platform-Based-on-Machine-Vision/index.html">
<meta property="og:site_name" content="Jordon-Xu&#39;s Blog">
<meta property="og:description" content="AbstractThe iteration of the automobile industry constantly promotes the rapid development of human economy and society. However, traffic jams and traffic accidents and other problems often cause trou">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/cover_2.png">
<meta property="article:published_time" content="2021-10-02T03:10:12.000Z">
<meta property="article:modified_time" content="2021-10-06T15:15:04.599Z">
<meta property="article:author" content="Jordon_Xu">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/cover_2.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/10/02/Automatic-Driving-Experiment-Platform-Based-on-Machine-Vision/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-06 23:15:04'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="Jordon-Xu's Blog" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/1.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">2</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">1</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Jordon-Xu's Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Automatic Driving Experiment Platform Based on Machine Vision</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-10-02T03:10:12.000Z" title="Created 2021-10-02 11:10:12">2021-10-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-10-06T15:15:04.599Z" title="Updated 2021-10-06 23:15:04">2021-10-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>The iteration of the automobile industry constantly promotes the rapid development of human economy and society. However, traffic jams and traffic accidents and other problems often cause trouble to People’s Daily travel. Therefore, more and more automobile manufacturers and technology companies begin to develop autonomous driving technology. Promoting autonomous driving cars has become the core development direction of the automobile industry.</p>
<p>First, this paper describes the process of building an autonomous driving experiment platform, as well as the software and hardware of the project, and the core design ideas and algorithms. Based on the chassis of the toy car, by using 3D model software to design the structure of the car, followed by printing out each part for assembly by 3D printer. The car contains on-board computer, suspension, differential, lighting, camera, distance sensor, and so on, having a high degree of hardware integration. At the same time, the car can be arbitrarily replaced or expanded through the modular idea. The road is simulated by grey foam panels, which can also be quickly built indoors and only covers an area of one square metre when storing.</p>
<p>Secondly, this paper introduces the idea of object - oriented design. Designing an event-driven real-time framework, and adopts the current mainstream machine vision and distance sensor method. In this paper, a new algorithm called “dynamic mask” is proposed to filter out non-pavement interference and predict the position of lane line.</p>
<p>Finally, this paper gives the autonomous driving state of the car in various typical road conditions, including keeping the middle of the lane, braking when encountering obstacles and passing through the intersection according to the traffic lights. Experimental results show that the proposed the autonomous driving platform based on machine vision is correct to a certain extent.</p>
<p>Key word: automatically driving, lane line detection, machine vision, real-time system, neural network</p>
<h1 id="Chapter-1-Introduction"><a href="#Chapter-1-Introduction" class="headerlink" title="Chapter 1 Introduction"></a>Chapter 1 Introduction</h1><h2 id="1-1-Project-Background"><a href="#1-1-Project-Background" class="headerlink" title="1.1 Project Background"></a>1.1 Project Background</h2><p>With the rapid development of science and technology, autonomous driving technology has attracted extensive attention of many scholars and related technology researchers. Legal and development costs make it important to have a small, suitable platform for autonomous driving. The author has developed a strong interest in the technical field of artificial intelligence and autonomous driving, and has determined computer engineering as the future major direction. The study will involve automation control, machine learning, software engineering and other fields. Therefore, an autonomous driving experimental platform for sustainable research has been designed. In long study life in the future, new knowledge, new algorithms and new ideas in these fields can be fully practiced and verified on this platform. At the same time, we also hope that the platform design can provide a reference solution for readers.</p>
<h2 id="1-2-Development-status-of-autonomous-driving-technology"><a href="#1-2-Development-status-of-autonomous-driving-technology" class="headerlink" title="1.2 Development status of autonomous driving technology"></a>1.2 Development status of autonomous driving technology</h2><p>Some developed countries began the research of driverless vehicles at the end of the 20th century. In the 1980s, famous universities in the United States, such as MIT and Carnegie Mellon University, started research on driverless vehicles. However, due to technical limitations, the project was gradually transformed into the assisted driving of civilian vehicles. In 1995, Navlab2 V, a driverless car developed by Carnegie Mellon University, was tested on a 5,000km interstate highway in the United States. Although the vehicle in the test only completed autonomous direction control, without speed control, it marked the basic success of the self-driving project. At the beginning of the 21st century, Toyota, Volkswagen and other companies have made achievements in assisted driving systems. In 2011, Google became the first company in the world to obtain the authorization of self-driving cars after Nevada passed the law on self-driving cars on roads [1]. In 2014, Tesla introduced its first generation of Autopilot, reaching L2 level of autonomous driving; In 2021, Tesla officially released the Full Self Driving (FSD), which abandoned the millimeter wave radar and adopted the algorithm of machine vision, and reached the L4 level of automatic Driving.</p>
<p>China’s research on driverless vehicles is later than foreign countries. In 1992, the National University of Defense Technology successfully developed China’s first truly driverless car. In 2007, National University of Defense Technology and FAW Jointly developed Hongqi flagship driverless car [2], whose technology has reached the world’s advanced level. Companies such as BYD, Changan and Baidu have made good progress in driverless research and development in recent years.</p>
<h2 id="1-3-Breakthroughs-and-accumulation-of-artificial-intelligence"><a href="#1-3-Breakthroughs-and-accumulation-of-artificial-intelligence" class="headerlink" title="1.3 Breakthroughs and accumulation of artificial intelligence"></a>1.3 Breakthroughs and accumulation of artificial intelligence</h2><p>In August 1956, a number of scientists are gathered to discuss problem that can be considered quite advanced: using the machine to mimic human learning and other aspects of intelligence, although the discussion did not have a result, but the concept of “artificial intelligence” was born [3]. Until now, artificial intelligence technology represented by deep neural network has developed rapidly with the promotion of hardware/computing power, data and algorithm. Deep neural network has crossed the “technology gap” between science and application [4], bringing the development climax to technologies such as autonomous driving and image recognition. So now, as a high school student, I can also stand on the shoulders of giants to enter the field of AI. I can build experimental platforms and complete engineering practices at home or in the primary laboratory at school.</p>
<p>The floating-point computing power of graphics cards is the hardware foundation for the development of artificial intelligence. NVIDIA’s CUDA architecture provides computing power for deep learning, which also promotes the development of autonomous driving technology. Before 2012, it was not common for people to use GPU for machine learning. Since 2012, AI computing power has increased by more than 300,000 times, and 10-100 GPU with the speed of 5-10 TFLOPS are commonly used for large-scale model training. According to OpenAI, human’s computing needs double every 3.43 months, increasing by about 10 times per year [5]. In 2021, the growth of GPU computing power will be similar to that of PC devices in the early years, and ordinary teams will be able to use GPU computing power to carry out complex machine learning projects. For example, the JETSON NANO used in this project has the computing power of 472 GFLOPs, but its size is only 80x100mm, and the cost is only 800 yuan. This kind of cost-effective hardware can only be obtained recently.</p>
<p>After 30 years of development, autonomous driving has accumulated a large amount of historical data, such as KITTI dataset, Audi open source autonomous driving dataset (2020), Waymo open source autonomous driving dataset (Google autonomous driving department) [7], etc.</p>
<p>Until 2000, deep networks (more than ten layers) were not trained in a more reliable way, but this changed during 2009-2016, when several simple but important algorithm improvements were made to optimize the backpropagation process of neural networks. In ILSVRC (ImageNet Large Scale Visual Recognition Challenge) competition, the best result of image classification was 26% error rate before 2012, but AlexNet reduced the error rate to 16% in 2012. ResNer increased its error rate to 9% from 25% in 2015, the previous year’s best score. Classical neural networks include AlexNet, VGG, ResNet, GoogleNet, YOLO[8], etc.</p>
<h2 id="1-4-Objective-and-significance-of-the-project"><a href="#1-4-Objective-and-significance-of-the-project" class="headerlink" title="1.4 Objective and significance of the project"></a>1.4 Objective and significance of the project</h2><p>The goal of this project is to build an autonomous driving experiment platform, which consists of three parts:<br>(1) Freely built simulated road sites and traffic signs;<br>(2) Modular model car;<br>(3) Software framework of sustainable iterative optimization.<br>The significance of this project is as follows:<br>(1) Realize the modularization of software and hardware (sustainable optimization iteration) : using 3D printing to realize the modularization of on-board computer, actuator drive board, camera, sensor and connector, which can easily expand and replace different hardware modules; Based on the object-oriented program framework, with maintainability, scalability.<br>(2) Low cost: The combination of ready-made toy parts and self-made 3D printed parts, as well as highly integrated electronic devices, can control the hardware cost of the project within a relatively low range.<br>(3) Small size: by limiting the volume of the car, the size of simulated site area is reduced, thus portable and fast construction is realized, which is very suitable for students.<br>(4) Compared with the simulation system, there are more opportunities for hardware practice: the design of the car involves the basic mechanical principles of the vehicle – driving form (front drive, rear drive, four-wheel drive), the role and form of suspension, the role of differential, the Ackermann steering geometry, etc. At the same time, electromagnetic compatibility of electronic components should also be considered – the adaptation of different control board operating voltage, system power demand, endurance, voltage regulator power supply (waveform), electromagnetic interference of power supply, etc.</p>
<h1 id="Chapter-2-Automatic-driving-experiment-platform-construction"><a href="#Chapter-2-Automatic-driving-experiment-platform-construction" class="headerlink" title="Chapter 2 Automatic driving experiment platform construction"></a>Chapter 2 Automatic driving experiment platform construction</h1><h2 id="2-1-Modular-design-of-model-car"><a href="#2-1-Modular-design-of-model-car" class="headerlink" title="2.1 Modular design of model car"></a>2.1 Modular design of model car</h2><h3 id="2-1-1-Trolley-chassis"><a href="#2-1-1-Trolley-chassis" class="headerlink" title="2.1.1 Trolley chassis"></a>2.1.1 Trolley chassis</h3><p>Using chassis of a toy car for transformation. The car has double wishbone suspension, driving form is rear drive (can be upgraded to four-wheel drive), replaced by dc motor drive with encoder; The original steering gear drive is retained, and the steering mechanism conforms to Ackermann steering geometry (a geometry that addresses the different centers of the vehicle’s inner and outer steering wheel paths when turning).</p>
<p>Closed-loop control of vehicle speed and travel distance is carried out. Speed is measured by Hall encoder – mechanical geometric displacement on the output shaft is converted into pulse or digital quantity through magnetoelectric conversion, and direction is judged by two groups of square wave signals with phase difference [9], as shown in Figure 2.1. The actual speed of the motor is fed back to the input to form a closed-loop control.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccUgWovqVSuX3wqpL0*EHUYr9lYLczxSMVvksLTGc4JFPpPMpUiRkaLEJua*JMY1srIsPpn8U8vIeCxR9CPCwqc!/b&bo=*gFAAf4BQAEBGT4!&rf=viewer_4" alt="Figure 2.1"></p>
<p>The maximum speed of the car is 150mm/s, and the minimum turning radius is 430mm.</p>
<h3 id="2-1-2-Vehicle-mounted-device-connection-module"><a href="#2-1-2-Vehicle-mounted-device-connection-module" class="headerlink" title="2.1.2 Vehicle-mounted device connection module"></a>2.1.2 Vehicle-mounted device connection module</h3><p>The connection module makes the car design become very flexible, only need to replace the interface parts so that different equipment can be replaced: such as car computer, motor steering drive board, sensor and so on. Figure 2.2 shows the 3D design of the two connection modules.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccUgWovqVSuX3wqpL0*EHUbXAUe9UXRHUfzykv6tdvzi8zjisiGXyZ337zINhxN1.6Bd7LTkZqEdyruPiwoiquY!/b&bo=4AGmAOABpgABGT4!&rf=viewer_4" alt="Figure 2.2"></p>
<h3 id="2-1-3-On-board-computer"><a href="#2-1-3-On-board-computer" class="headerlink" title="2.1.3 On-board computer"></a>2.1.3 On-board computer</h3><p>The on-board computer is mainly responsible for the calculation of the core algorithm and the implementation of the software framework. As shown in Figure 2.3, the core “brain” of the car uses NVIDIA JETSON NANO, which contains CPU with 4-core ARM architecture, GPU with 128-core Maxwell architecture (with 472GFLOPs computing power), and 4GB LPDDR4 memory.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccUgWovqVSuX3wqpL0*EHUbGw2nZbLYCTPtik0s2fgIdcCvblhjuhShrMwhAyxMgowyOyRH5bMlXomN8H3T9dV4!/b&bo=fAICAnwCAgIBGT4!&rf=viewer_4" alt="Figure 2.3"></p>
<p>The biggest feature of this development board is that it contains a GPU with 128 core Maxwell architecture, which basically meets the minimum requirements of parallel computing for lane detection, object detection and basic image processing in this project. Among them, size is its biggest advantage, it can be easily integrated in a variety of embedded applications; The power consumption is also very low, 20W in high power mode.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcaG3OvQGv5sF1GzJxn158F7b*M5114iyAwEdNTay1lw11u6t9g0WwTuaWBJy5Mc1CLTmhWOAAQIX*AGW0EaKnkI!/b&bo=zgKuAc4CrgEBGT4!&rf=viewer_4" alt="Figure 2.4"></p>
<h3 id="2-1-4-Motor-driver-board"><a href="#2-1-4-Motor-driver-board" class="headerlink" title="2.1.4 Motor driver board"></a>2.1.4 Motor driver board</h3><p>The Motor driver board is the driving equipment of the executive mechanism. It communicates with the on-board computer through I2C protocol and drives the motor and steering gear using PWM principle.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mceOPcKgNysQL3J3qZe5aMTonoBT8V2fsIySygZAZrkiuENuFzL4zwAE2iTlhCHqvytBYX3D2FALjQddl.6AKM*8!/b&bo=AAHKAAABygABGT4!&rf=viewer_4" alt="Figure 2.5"></p>
<p>The speed control principle of the motor is pulse-width modulation (PWM), with two important parameters: frequency and duty cycle. The frequency is the reciprocal of the period; Duty cycle is the proportion of high level in a cycle [10]. The schematic diagram of PWM square wave is shown in Figure 2.6.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mceOPcKgNysQL3J3qZe5aMTo*0tvzLU0u*p*bm1VusI1UT.HE8QDrEPweCI*I1bUFJsk9cVed.*RTxjl6otMqCvc!/b&bo=*gCAAP4AgAABGT4!&rf=viewer_4" alt="Figure 2.6"></p>
<p>In Figure 2.6, frequency F has a value of 1/(T1+T2) and duty cycle D has a value of T1/(T1+T2). Frequency modulation can be realized by changing the number of pulses per unit time. Pressure regulation can be achieved by changing the duty cycle. The larger the duty cycle is, the larger the average voltage and the larger the amplitude will be. The smaller the duty cycle, the smaller the average voltage and the smaller the amplitude [11].<br>The angle control of the steering gear is also produced by the continuous pulse. The length of the pulse determines the rotation angle of the steering gear. For example, the 1.5ms pulse will make the steering gear rotate to the middle position (for the 180° steering gear, the 90° position). When the control system commands the steering gear to move to a certain position and hold it at the same angle, the external force will not change its angle, but there is an upper limit, the upper limit is its maximum torque. Unless the control system continuously sends pulses to stabilize the angle of the steering gear, the angle of the steering gear will remain constant [11]. Figure 2.7 shows examples of steering gear rotation at different pulse widths.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcaG3OvQGv5sF1GzJxn158F7I*mbbgT5XgXubXPAiYetw958YxBpk4SchPgkUgnhD6VwYnlY1.*AjwdP9g.J.PYQ!/b&bo=tAR8ArQEfAIBGT4!&rf=viewer_4" alt="Figure 2.7"></p>
<h3 id="2-1-5-camera"><a href="#2-1-5-camera" class="headerlink" title="2.1.5 camera"></a>2.1.5 camera</h3><p>As shown in Figure 2.8, it is IMX219 series camera, the author tried the 77 - degree view angle at first, it was found that field is not wide, some lane lines can’t enter the picture, in the end,  choosing a 200 - degree view angle camera. Due to large view angle, distortion is inevitable, that will influence object recognition. Therefore, procedures are used to correct the distortion of the camera.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mceOPcKgNysQL3J3qZe5aMTp66zLqt*r19A1UcsbMy4Zt6877yuP*FoP*GyN**KCnPAY5oshFzqfWvwG*dHJMdPs!/b&bo=wgCwAMIAsAABGT4!&rf=viewer_4" alt="Fiure 2.8"></p>
<p>In this paper, the checkerboard-based camera distortion correction method is adopted, whose essence is to realize the calculation of camera parameters by finding the corresponding relationship between the corner points on the checkerboard in the image and the real world [12]. First, the checkerboard pictures were taken from different angles using a camera that needed to be corrected. Then, the findChessboardCorners function provided by OpenCV was used to find the corner points combined with black and white boxes in the checkerboard, and generate three-dimensional space points in the world coordinate system. Then, the corresponding camera parameters are calculated by calibrateCamera function to realize camera calibration. Finally, undistort, the most basic distortion calibration function provided by OpenCV, is used to complete calibration. The comparison before and after calibration is shown in Figure 2.9 and Figure 2.10.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mceOPcKgNysQL3J3qZe5aMToSdzkf5fUqLFhiR.o1zPVU6YXGdxuTXIIF2mdN7mbSBLh.pW45l8ChAHicPhsQGyY!/b&bo=gAEMAYABDAEBGT4!&rf=viewer_4" alt="Figure 2.9"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcbip6LoRKlwSROv4Yj9z9d6dbkYOt51G.AYNt2QgCAyw5mkENu7MaWKn2WVuS42Ek52yAgyqO1RtViYW.WDT71g!/b&bo=bgEOAW4BDgEBGT4!&rf=viewer_4" alt="Figure 2.10"></p>
<h3 id="2-1-6-Range-sensor"><a href="#2-1-6-Range-sensor" class="headerlink" title="2.1.6 Range sensor"></a>2.1.6 Range sensor</h3><p>Driverless cars mainly rely on vision, which can be combined with distance sensors to achieve more accurate identification. This scheme uses ultrasonic distance sensor and infrared distance sensor, in which the ultrasonic distance sensor is used to measure the distance in front of the car, and four groups of infrared distance sensors are used to detect whether there are obstacles around the car.</p>
<h3 id="2-1-7-Battery-and-voltage-regulator-power-supply"><a href="#2-1-7-Battery-and-voltage-regulator-power-supply" class="headerlink" title="2.1.7 Battery and voltage regulator power supply"></a>2.1.7 Battery and voltage regulator power supply</h3><p>The car power supply system uses four 18650 lithium batteries (3.4a.h ×3.7V×4= 50.32w.h) and 5V8ADC-DC synchronous step-down module (high efficiency 96%, low ripple &lt; 30mV).</p>
<table>
<thead>
<tr>
<th>Hardware</th>
<th>Power</th>
</tr>
</thead>
<tbody><tr>
<td>On-board computer</td>
<td>10W</td>
</tr>
<tr>
<td>Motor driver board</td>
<td>Motor 0.6W Steering gear 1W</td>
</tr>
</tbody></table>
<p>It can be calculated from Table 2.1 that the total power of the trolley is about 11.6W and the running time is about 4 hours.</p>
<h2 id="2-2-Design-of-simulated-experimental-site"><a href="#2-2-Design-of-simulated-experimental-site" class="headerlink" title="2.2. Design of simulated experimental site"></a>2.2. Design of simulated experimental site</h2><h3 id="2-2-1-Design-and-splicing-of-experimental-roads"><a href="#2-2-1-Design-and-splicing-of-experimental-roads" class="headerlink" title="2.2.1 Design and splicing of experimental roads"></a>2.2.1 Design and splicing of experimental roads</h3><p>In order to simulate the real road, gray sponge foam splicing pad is used to simulate the real road surface, and white tape is used to simulate the real lane lines. This site is composed of more than 30 pieces of foam, which can be spliced in a short time, and can be folded up when not in use, so that the modality of the simulated site can be completed in a narrow space. As shown in Figure 2.11 and Figure 2.12, the test site can be rapidly deployed indoors, covering approximately 40 square meters.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcaG3OvQGv5sF1GzJxn158F6CmfFYN0liYUPOC2POumBQ3EpmEr51EJ9DXRJuoIalES8XVfTjjT4FPizBHocuhHE!/b&bo=bgEgAm4BIAIBGT4!&rf=viewer_4" alt="Figure 2.11"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcaG3OvQGv5sF1GzJxn158F6bfXNDyfpNDw3.iHMEDNihiQxR4MW5PvEAiz6R*vsW8EDeEOZ6kcAF3B.xo8bIooU!/b&bo=ogO6AqIDugIBGT4!&rf=viewer_4" alt="Figure 2.12"></p>
<h3 id="2-2-2-Simulate-traffic-signs-vehicles-and-pedestrians"><a href="#2-2-2-Simulate-traffic-signs-vehicles-and-pedestrians" class="headerlink" title="2.2.2 Simulate traffic signs, vehicles and pedestrians"></a>2.2.2 Simulate traffic signs, vehicles and pedestrians</h3><p>In this paper, model traffic signs, toy cars and model pedestrians are purchased from the Internet to simulate real road scenes, as shown in Figure 2.13.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcZ04UnZxuRIDvxt4rhEnR8fKy.pD0Z7e5FnyYe5lKkEedMVlx16oYdZvntibxVp5TZVFMwHLG5tFlkcbk6Hv7Xo!/b&bo=aAR0AmgEdAIBGT4!&rf=viewer_4" alt="Figure 2.13"></p>
<p>#Chapter 3 Software framework and core algorithm design</p>
<h2 id="3-1-Software-Environment"><a href="#3-1-Software-Environment" class="headerlink" title="3.1 Software Environment"></a>3.1 Software Environment</h2><p>The operating system used in this project is Ubuntu of Linux and the programming language is Python. Table 3.1 lists the versions of the core software in the project.</p>
<table>
<thead>
<tr>
<th>Software</th>
<th>Version</th>
</tr>
</thead>
<tbody><tr>
<td>Ubuntu</td>
<td>18.04</td>
</tr>
<tr>
<td>Python</td>
<td>3.6.9</td>
</tr>
<tr>
<td>Cuda</td>
<td>10.2</td>
</tr>
<tr>
<td>Numpy</td>
<td>1.19.1</td>
</tr>
<tr>
<td>Tensorrt</td>
<td>7.1.3.0</td>
</tr>
<tr>
<td>Opencv4</td>
<td>4.1.1</td>
</tr>
</tbody></table>
<h2 id="3-2-Event-driven-real-time-system-framework"><a href="#3-2-Event-driven-real-time-system-framework" class="headerlink" title="3.2 Event-driven real-time system framework"></a>3.2 Event-driven real-time system framework</h2><p>From the perspective of learning software engineering development process and object-oriented design ideas, this project does not use mature robot framework such as ROS, but independently builds a lightweight real-time system framework.<br>The software design is implemented in strict accordance with the standardized software engineering workflow: Requirement analysis → Test case→Outline design→Detailed design→Code→Test.</p>
<h3 id="3-2-1-Practice-the-idea-of-object-oriented-programming-through-self-built-framework"><a href="#3-2-1-Practice-the-idea-of-object-oriented-programming-through-self-built-framework" class="headerlink" title="3.2.1 Practice the idea of object-oriented programming through self-built framework"></a>3.2.1 Practice the idea of object-oriented programming through self-built framework</h3><p>As the project needs continuous optimization and iteration, it is required to have reliability, maintainability and scalability. The purpose is that when the requirements change, the program structure design has rationality, the code change momentum is small, the impact on the overall structure is small, the possibility of error is small, in order to improve the efficiency of project development.</p>
<p>Design Pattern – GoF, “is a set of repeatedly used, widely known, classified and catalogued summary of code design experience [13]. Design pattern is used to reuse code, make it easier for others to understand, and ensure that code is reliable. Design pattern makes coding truly engineering and is also the cornerstone of software engineering, just like bricks and stones of a mansion. Only by mastering design pattern can one truly understand software engineering [14].” Design pattern is abstractions of things in the real world and follow six principles: Single Responsibility Principle (SRP), Liskov Substitution Principle (LSP), Dependence Inversion Principle (DIP), Interface Segregation Principle (ISP), Law of Demeter (LoD), Open Close Principle (OCP).</p>
<p>In a program, encapsulation and polymorphism of classes serve the schema, which ultimately serves the framework. Therefore, a framework is typically an abstraction of multiple schemas. As far as this project is concerned, abstract modeling is carried out through the requirements of the real physical world of the autonomous driving simulation platform. The resulting model should be a mapping of the “nature” of the real physical world (i.e. their properties and logic are equivalent), abstracting the following main patterns:<br>(1) Singleton pattern<br>The singleton pattern is defined as ensuring that a class has only one instance and providing a global access point to access it [28]. The model design process is from easy to difficult. In this project, lane line detection, object detection, car actuator and so on are all single algorithms and logic at present, so each function only needs to encapsulate a class, and the life cycle of these objects is the same as that of the framework. So the singleton pattern here simplifies the framework because these objects only need to be created and destroyed once, fulfilling the guidelines of high cohesion and low coupling.</p>
<p>(2) Observer pattern<br>The observer pattern is a pattern of object behavior. It defines a one-to-many dependency between objects. When the state of an object changes, all objects dependent on it are notified and automatically updated [28].</p>
<p>The reason for considering observer pattern in this project is that the program framework is event-driven (Figure 3.2). The main events include:</p>
<ul>
<li>Timer events;</li>
<li>Lane line events generated by machine vision;</li>
<li>Object detection events generated by machine vision;</li>
<li>Spatial events generated by the distance sensor;</li>
<li>Counting event of speed encoder;</li>
<li>Driving control events (steering angle and speed setting).</li>
</ul>
<p>These events are processed and output by different classes (producers), and used by the self-driving logic class (consumers). The producer doesn’t have to care about the exact caller of the event, and the consumer doesn’t have to know where the event came from, just tell the framework which events “I” care about (event registration) and get those events in time (event message distribution). This follows the Law of Demeter of only talking to your immediate friends and not to “strangers” [14]. The essence of Law of Demeter is that if two software entities do not have to communicate directly, then direct calls to each other should not occur and can be forwarded by a third party. Its purpose is to reduce the degree of coupling between classes and improve the relative independence of modules. For example, the lane detection class, the object detection class, the timer and the state machine are “strangers”, and no direct calls (low coupling) occur between them. Because they are in different threads, if they call each other’s interfaces directly, the complexity and debugging difficulty of the program will increase greatly for thread-safety reasons, and the execution efficiency will not increase, so this is wrong and completely unnecessary.</p>
<p>(3) State pattern and chain-of-responsibility pattern<br>The state pattern wraps the behavior of the studied object in different state objects, each of which belongs to a subclass of an abstract state class. The intent of the state pattern is for an object to change its behavior when its internal state changes. The state pattern requires the creation of a state class subclass for each state that the system can achieve. When the state of the system changes, the system changes the selected subclass [32].</p>
<p>Chain-of-responsibility pattern is a software design pattern in object-oriented programming, which contains some command objects and a series of processing objects. Each processing object determines which command objects it can process, and it knows how to pass command objects that it cannot process to the next processing object in the chain [30].</p>
<p>This project will need to adapt to a variety of simulation under the condition of automatic driving, which requires abstraction of the control logic, which the complex scene as far as possible is decomposed into several simple situations, then each scene abstract to concrete implementation control logic for the state of the corresponding class, each state only “focus” to deal with a control logic scenarios (Single Responsibility Principle). The process of human driving is also based on the current different scenes to make corresponding driving operations, so this abstraction itself is “natural” and relatively simple, as long as considering oneself in driving (the author is a minor, but have the game simulation driving experience).</p>
<p>Because the change of road condition itself is continuous, the “natural” abstraction is the continuous migration of corresponding state. Here is different from the real world, each state of the model can be seen as a “responsible” (In reality, of course, only one person is driving. This is to abstract the driving skills of natural persons and “split” them to the “responsible person” in each program)  each “responsible” is only good at driving logic of specific scenarios, And know who is the next “responsible person” to be designated after the current scene changes, so that the state transfer of the state machine is completed, and the “chain-of-responsibility pattern” is applied to realize the continuity and integrity of the control logic. In this project, “cruise state” and “traffic light state” are designed. In the “cruise state”, only responsible for the car’s normal line patrol and obstacle avoidance; When the traffic light is detected (reaches a certain distance), it switches to “waiting traffic light state”; The car starts to slow down when detecting the intersection stop line and red light, it stops and waits until it detects the green light and switches to the next state: “straight state” or “turn state”.</p>
<p>When abstracted into these two patterns, each state class has a single responsibility. If the control defects are found in a certain scene during debugging, only one state class corresponding to the optimization can be modified, which will not affect the other parts, so as to meet the thought of high cohesion and low coupling program design.</p>
<p>In the framework of the top, it don’t need a scheduling module of processing logic according to different road conditions, only in the message loop continuously distributed to register the event interface, this is because the control logic has been through the realization of the “chain-of-responsibility pattern” implicit control scheduling, namely “Liskov Substitution Principle” and “Dependence Inversion Principle” of the practical application. The top level of the framework is only responsible for switching the state machine and forwarding the registered event messages to the current state, and all states inherit from the same state parent class, so the top level of the framework only calls its virtual methods, and the specific state object is transparent to the top level of the framework. No matter how you add, delete, or change these states, there is no change at the top level of the framework.</p>
<p>If a new simulation road scene (research project itself is a from simple to complex, increasing demand process), only need to add new state subclasses, and modify the superior “responsible” to make it to yourself, rest of the framework is unchanged, it is to follow the “Open Close Principle (open for extension, but closed for modification)”. For example, if you want to realize the car overtaking function in this project, you only need to add the category of “overtaking state” to realize a series of car functions, and add the condition of switching to “overtaking state” in the category of “cruise state”, without changing the other parts of the program.</p>
<p>In the process of the above abstract modeling, Open Close Principle, Liskov Substitution Principle and Dependence Inversion Principle are used. In general, these three design principles will appear at the same time. The Open Close Principle is the goal, Liskov Substitution Principle is the foundation, and Dependence Inversion Principle is the means. The three principles complement each other and have the same goal [13].</p>
<h3 id="3-2-2-Multithreading-structure"><a href="#3-2-2-Multithreading-structure" class="headerlink" title="3.2.2 Multithreading structure"></a>3.2.2 Multithreading structure</h3><p>Processes are the smallest unit of resource allocation and threads are the smallest unit of CPU scheduling. Multiprocesses occupies a large amount of memory, complex switchover, and low CPU usage. Multithreading occupies less memory, has simple switching and high CPU utilization [31]. There are multiple core algorithms in this project that consume significant processor resources and time, as shown in Figure 3.2.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcQd4lKPu7Ogg.cQzdptXZCyoiavb77zn6SJKLTgEKH16iFEwiqN7KRMbHxCi86UK56ThxZHZXilZTOKVFA0l83o!/b&bo=HgPCAR4DwgEBGT4!&rf=viewer_4" alt="Figure 3.2"></p>
<p>As can be seen from Figure 3.2, each frame of the video needs 10ms time processing, lane detection needs 50ms and object recognition and detection needs up to 250ms. Sensor polling, control logic and so on all need a certain time interval to calculate.</p>
<p>In this project, when the processor takes 10ms to process the received frame, it needs 50ms and 250ms respectively to detect lane lines and identify objects, and then use the detection results to make logical judgment and control the car. During 310ms detection, all the resources of the processor are allocated to deal with time-consuming serial tasks, and the line patrol logic that depends on lane events can only be executed 3 times per second on average, so the whole system will be blocked by no less than 310ms, and the car patrol response is seriously lagging behind, so it cannot run normally. Using a multithreading concurrent event generation mechanism, tasks are executed in separate threads. As shown in Figure 3.2, four processors core tasks are in parallel processing, after the creation of each individual event will be pressed into an event queue (thread safety), the main thread cyclic polling all the events in the queue, and distributed to the current state of the interface, so the main thread can handle 20 times lane line detection event, 4 times object detection events and 40 times distance sensor event per second. So that the car can respond in time, and drive normally.</p>
<h3 id="3-2-3-Framework-design"><a href="#3-2-3-Framework-design" class="headerlink" title="3.2.3 Framework design"></a>3.2.3 Framework design</h3><p>(1) Class diagram<br>The class diagram shows the internal structure of each class in the model and its relationship to other classes.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcQd4lKPu7Ogg.cQzdptXZCzXdxfbWoEa1FemLKAbdy8WavAjEPTjF2bdi0wAGrEzzpaN2Y2As.hE5YFJc5MKXYI!/b&bo=zgJ2As4CdgIBGT4!&rf=viewer_4" alt="Figure 3.3"></p>
<p>Figure 3.3 shows the different classes in the project’s framework and their relationships:</p>
<ul>
<li>Scheduler: The main thread driven, which is the core of the upper scheduling class and the overall control. The main function is to receive and distribute events to listeners for processing, and to control state switching.</li>
<li>LaneDection: Child thread driven, created and destroyed by Scheduler. The main function is to generate lane line events. LaneDection first updates the image with update_image() and then sends the finished lane-line event as a message to the Scheduler.</li>
<li>Camera: Child thread driven, created and destroyed by the Scheduler. The Camera sends the Scheduler each frame of the video.</li>
<li>ObjectDetection: Child thread driven, created and destroyed by Scheduler. The main function is to generate object detection events. ObjectDetection first updates the image with update_image() and then sends the processed object detection event as a message to the Scheduler.</li>
<li>DisDetection: Child thread driven, created and destroyed by Scheduler. The main function is to generate distance detection events. The polling method is adopted to send the distance detection event as a message to Scheduler .</li>
<li>Queue: event 1ueue (first-in, first-out), created and destroyed by the Scheduler. Because of its internal encapsulation of the thread safety mechanism, so the concurrent events of multiple threads into serial processing.</li>
<li>Car: The main function is control of the Car, which is created and destroyed by Scheduler. It is controlled by two main parameters: speed and steering angle.</li>
<li>StateBase: the parent class of the state mode, and is used to implement interface-oriented programming of its subclasses through abstract three virtual methods: Enter, Do, and Exit.</li>
<li>StateTest: the subclass that inherits StateBase. The main function is to realize the self-inspection of the trolley before performing the task.</li>
<li>StateCruise: the subclass that inherits StateBase. The main function is to realize all the tasks under the trolley cruise state.</li>
<li>StateWaitTrafficLight: the subclass that inherits StateBase. The main function is to realize all the tasks of the trolley under the waiting red light state.</li>
<li>StateCrossRoad: the subclass that inherits StateBase. The main function is to realize all the tasks under the state of the car crossing state.</li>
<li>StateTurn: the subclass that inherits StateBase. The main function is to realize all tasks under the left/right turn state of the trolley.</li>
<li>Expending: there are two classes in grey in the picture above, indicating that this area can be expanded at a later stage. For example, if a new function needs to be implemented, you can simply add a new class or a new class that extends from StateBase, while the rest of the code does not need any modifications and implements the Open Closed Principle.</li>
</ul>
<p>Virtual function is a means of realizing polymorphism. The purpose is to separate interface from implementation for interface oriented programming. It is only at runtime that the program decides which function should be called. No matter which class object is passed, the corresponding method can be obtained through the same interface. When demand change, such as the car needs to cope with a new road, just write to implement the requirements of the new class, by changing the framework of the interface implementation class demand, which can be completed without the need to make changes in the original code, so that it can reduce the coupling of the code. To reduce code coupling, maximize decoupling, implement the Open Closed Principle mentioned above.</p>
<p>(2) State diagram<br>As shown in Figure 3.4, the state diagram shows the conditions and logic for the intertransformation of five states.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcV94nIU0m7aXgZ5RX9ac7x4ak1RBX446B1r03bCi9KDAmhZZdWQwzoQS7n4lqycoVwh3ihcQzMl3zKDcFMkZDuQ!/b&bo=lAL0AZQC9AEBGT4!&rf=viewer_4" alt="Figure 3.4"></p>
<p>When the car is running, it first enters the StateTest. After the Test Over, it enters the StateCruise and starts to run. When the car meets a Green Light, it switches to the StateCrossRoad or StateTurn state. When the car meets a Red Light, it enters the StateWaitTrafficLight, and when the light turns green, it switches to straight or turning state. When  the lane is recaptured, the car switches to cruise again.</p>
<h2 id="3-3-Core-algorithm-–-Dual-channel-inference-tracking-lane-line-detection-algorithm"><a href="#3-3-Core-algorithm-–-Dual-channel-inference-tracking-lane-line-detection-algorithm" class="headerlink" title="3.3 Core algorithm – Dual channel inference tracking lane line detection algorithm"></a>3.3 Core algorithm – Dual channel inference tracking lane line detection algorithm</h2><p>The lane line is different from the asphalt road, it has a very obvious feature, that is, its color is white. In this project, gray sponge pad is used to simulate asphalt road, and white tape is used to simulate lane line. Based on the current environment perception, this algorithm deduces and tracks lane lines through lane line prior features. The steps are as follows:<br>(1) Grayscale transformation: use OpenCV method cvtColor to convert RGB images into grayscale images for post processing.</p>
<p>(2) Gaussian filtering: the image will be polluted by many random signals and become noise. Some common noise has pepper noise, pulse noise, gaussian noise and so on. Pepper noise contains random black and white intensity values, while pulse noise contains only random white intensity values (positive pulse noise) or black intensity values (negative pulse noise). Gaussian noise includes noise whose intensity obeys Gaussian or normal distribution [15].<br>Gaussian smoothing filter is very effective in removing Gaussian noise, of course, it is also effective in removing other noises in most cases. Gaussian filtering is to add a Gaussian smooth filter to an image to achieve the effect of eliminating noise. The basic principle is to take the points near each point on the image for weighted average recalculation value. Gaussian smoothing filter is a low pass filter, which can suppress the high frequency part of the image and only allow the low frequency part to pass through [16]. For example, when taking photos in a dark environment, ISO parameters will be larger, resulting in poor picture quality and more noise points, which are the high-frequency part of the picture.</p>
<p>(3) Edge detection: as the name implies, it is to detect the boundary of the image object, so that the lane line can be correctly identified behind. The image has been transformed into grayscale image in (1) grayscale transformation. The grayscale value of each pixel is between 0 and 255. There is a great difference between lane line and road surface, so color mutation can be used for detection.</p>
<p>(4) Calculating the gradient: the brightness of each pixel corresponds to the gradient intensity of the point, so the edge can be obtained by tracking the pixel under the maximum gradient; Get the shape of an object by identifying its edges.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcQd4lKPu7Ogg.cQzdptXZCxZ4UToWMEF8xwZqmUrBNMWox9JgbnQ2*Cc5NQCaetS117gR5BAabGFH0Ez6R77H60!/b&bo=GAIqARgCKgEBGT4!&rf=viewer_4" alt="Fiure 3.5"></p>
<p>The change curve in Figure 3.5 shows the change of gray value in the red line, and it can be seen that points A, C and E are most likely to be identified as lane line edges. The algorithm determines whether a point is an edge point by comparing the gradient and the relationship between the two thresholds. If the gradient is greater than the high threshold, it determines that it is an edge point, and vice versa.</p>
<h3 id="3-3-1-Optimized-dual-channel-inference-tracking-ROI-algorithm"><a href="#3-3-1-Optimized-dual-channel-inference-tracking-ROI-algorithm" class="headerlink" title="3.3.1 Optimized dual channel inference tracking ROI algorithm"></a>3.3.1 Optimized dual channel inference tracking ROI algorithm</h3><p>After the operations described in Section 3.3, it is possible to detect the contour of the object, but it is found that the detected edges include not only the edge of the lane line, but also the edge of many other objects, among which it is very difficult to determine which edge is the lane line. As car camera relative to the car are fixed, and the car relative to the position of the two lane lines has little difference, so the lane line in the field of vision to remain in a fixed area, so we can draw a rough lane line in ROI (Region of Interest), filter out the disturbance of other area.</p>
<p>Set the identified area as a fixed trapezoid ROI (currently the mainstream scheme), but in the subsequent debugging process, it was found that there were still many internal disturbances in trapezoid (for example, close vehicles and pedestrians), and lane lines would appear outside the trapezoid area in the case of sharp turn.</p>
<p>The improvement goal is to cover the lane lines with the ROI as small as possible. After analysis, the following simple rules are found:</p>
<ul>
<li>Apparently under normal conditions (not broken), there are 2 lane lines;</li>
<li>The average interval between lane lines is a constant value;</li>
<li>Lane line (perspective) slope and position changes are continuous;</li>
<li>The smaller the ROI area, the less the interference.</li>
</ul>
<p>According to the above rules, the ROI region is optimized from a fixed trapezoidal region to a “dual channel inference tracking” region, using the following method:</p>
<ul>
<li>Dual channel: set two independent parallelogram ROI areas with the minimum area and the same slope as the lane line, whose area is much smaller than the trapezoid to reduce interference.</li>
<li>Dynamic tracking: The position and slope of the ROI area of the parallelogram are dynamically updated, consistent with the single lane line identified in the current frame. When the image is updated, because the changes are continuous, the lane lines must fall within the dynamic ROI region. In this way, the ROI is kept as small as possible without the lane lines falling outside the area.</li>
<li>Inference: There are at least two situations in which a single lane line can be “lost”, either when the lane line is obscured or damaged by an object, or when the lane line is out of view during a curve. At this time, another advantage of “dual channel” will come into play. As long as one lane line is successfully identified, the approximate position of the other lane can be “inferred” by the rule that “the average interval between lane lines is a constant value”, and “wait” for it to return at this position.</li>
</ul>
<p>After using the dynamic ROI mechanism, the recognition success rate is high and the anti-interference ability is strong, which greatly improves the robustness of the algorithm. The first step of the algorithm is to create the initial ROI mask and calculate the coordinates of the eight vertices of the mask (4 coordinates for each mask) according to the image size. The second step is to update the next mask according to the identified lane lines. The following mask is based on the position of the current lane line. If one lane line is unrecognizable or out of view of the camera, the program can still predict the position of the other lane line (calculate the approximate area of the other lane line by assuming that the two lanes are equidistant).</p>
<p>3.3.2 Hough transform<br>Hough transform is a feature detection used to detect objects with a particular shape, such as a straight line or circle. Its principle is to insinuate the original space into the parameter space and vote in the parameter space to obtain the desired graph [17].</p>
<p>The line in Figure 3.6 can be represented as y=m0x+b0, which is a point in hough (parameter) space.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcYpimNcxs30tx7jmFBCNFT8VvSHjfqvXGH4TgbYw.qKeFSgrIw.rVuV82iUEjU81Oc0fC6u0Cq2kqxtUoZp33rY!/b&bo=8AH8APAB*AABGT4!&rf=viewer_4" alt="Figure 3.6"></p>
<p>On the contrary, points in Figure 3.7 can be represented as lines in Hough space. To detect lines, each point in the image can be converted to Hough space and parameters m0 and b0 can be determined by finding points where lines intersect in Hough space.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mceVHlpiOZOWahjQoixLh666j8pGp2WKWv2AhYFLsvpqBFhBXlP*i8w1SgxnZs0jNgujL7jfvWcBPItyPExpaBoU!/b&bo=ygHUAMoB1AABGT4!&rf=viewer_4" alt="Figure 3.7"></p>
<p>Points in the original space correspond to curves in the parameter space, and lines in the original space correspond to intersections of curves in the parameter space. Then, if there are multiple lines intersecting a point in Hough space, the corresponding points in the image space should be located on the same line, as shown in Figure 3.8.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mceVHlpiOZOWahjQoixLh665HaI8TnETO5oFA4D3XirGriaOOQJ8cfjL*SPGsEZdfZrHrAYalH.OUWsMFgi.*zXk!/b&bo=8gHoAPIB6AABGT4!&rf=viewer_4" alt="Figure 3.8"></p>
<p>Function CV2. HoughLinesP is a probability linear detection, and Hough transform is a time-consuming and labor-intensive algorithm. Even after Canny transformation, the number of points is still huge. In this case, a probability selection mechanism can be adopted, that is, not all points are calculated, but some points are randomly selected for calculation, which is equivalent to down-sampling [18].</p>
<p>The HoughLinesP function is used with the following parameters [25] :</p>
<ul>
<li>Image: is a binary image, and the remaining image in the mask is used as input.</li>
<li>Rho: is the resolution of the parameter pole diameter R in pixel value. The recommended value 1.0 (1 pixel) is used here.</li>
<li>Theta: is the resolution of the parameter polar angle θ in radians, and the recommended value is numpy. PI /180 (1 degree).</li>
<li>Threshold: indicates the minimum curve intersection required to detect a straight line. After debugging, this parameter is set to 15.</li>
<li>lines: the container storing the parameter pairs (x_{start}, y_{start}, x_{end}, y_{end}) of the detected line, that is, the coordinates of the two endpoints of the line segment.</li>
<li>minLineLength: is the minimum number of points that can form a straight line (lines with insufficient points will be discarded). After debugging, this parameter is set to 40.</li>
<li>maxLineGap: two line segments in the same direction are judged as the maximum allowable interval and fracture of one line segment (if the value exceeds the set point, the two line segments are regarded as one line segment; the larger the value is, the larger the allowable fracture of the line segment is, and the more likely the line segment is to be detected). After debugging, this parameter is set to 20.</li>
</ul>
<h3 id="3-3-3-Extraction-and-fitting-of-lane-lines"><a href="#3-3-3-Extraction-and-fitting-of-lane-lines" class="headerlink" title="3.3.3 Extraction and fitting of lane lines"></a>3.3.3 Extraction and fitting of lane lines</h3><p>Lane lines have been detected, and the last is to calculate and fit the correct lane lines according to the detected points. Determine whether a certain line belongs to the left lane line or the right lane line according to the positive and negative slope, buffer the mean sampling of three detection results, calculate the difference between the slope of each line and the mean slope iteratively, and remove the line with excessive difference. Finally, the np. polyfit method is used to fit the function expression.</p>
<h2 id="3-4-Core-algorithm-–-YOLO-target-detection"><a href="#3-4-Core-algorithm-–-YOLO-target-detection" class="headerlink" title="3.4 Core algorithm – YOLO target detection"></a>3.4 Core algorithm – YOLO target detection</h2><h3 id="3-4-1-YOLO-V4-network-structure"><a href="#3-4-1-YOLO-V4-network-structure" class="headerlink" title="3.4.1 YOLO-V4 network structure"></a>3.4.1 YOLO-V4 network structure</h3><p>YOLO-V4 algorithm is based on the original YOLO target detection architecture and adopts the most excellent optimization strategy in the CNN field in recent years. There are different degrees of optimization in data processing, backbone network, network training, activation function, loss function and other aspects. The target detection detector consists of four parts: Input, Backbone, Neck, and Head[19].<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcV94nIU0m7aXgZ5RX9ac7x6jvhOQfNskBk0Bi8oMKv0mcm7BuLMmB9dbXh*WRCwZlFlTMBw96YRGnaeP5FrG2q8!/b&bo=cAIuAXACLgEBGT4!&rf=viewer_4" alt="Figure 3.9"></p>
<p>(1) Input: indicates the Input.<br>(2) Backbone: refers to the convolutional neural networks with pre-training parameters used for feature extraction, such as VGG16 and RESNET-50.<br>(3) Head: is the detection Head, which is mainly used to predict the type and position of the target.<br>(4) Neck: is the network layer between Backone and Head for collecting feature maps in different stages. [19]</p>
<p>When the input is 416*416, the YOLO- V4 network structure is shown in Figure 3.10.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcV94nIU0m7aXgZ5RX9ac7x47rqsMJwqKyhbkiYErgBCaLAVhCJpsS5RAZh4yYhMYt4y.vU4mHc63wqaVfl1KUbg!/b&bo=qALeAagC3gEBGT4!&rf=viewer_4" alt="Figure 3.10"></p>
<p>As an improved version of YOLO-V3, YOLO-V4 retains the original Head, while the trunk feature network is modified from DarkNet53 to CSPDarkNet53, and SPP (spatial pyramid pooling) and Mish activation functions are used to increase the sensory field.</p>
<h3 id="3-4-2-YOLO-network-training"><a href="#3-4-2-YOLO-network-training" class="headerlink" title="3.4.2 YOLO network training"></a>3.4.2 YOLO network training</h3><p>The activation function used in YOLO-V4 is Mish=x × tanh(ln(1+e^x)), as shown in Figure 3.11.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcYl9tU*h3xDSuMuRgI1dmqKqnTWsJLMYZhUCoRYuU3NsxdbrmkL7ugryZv.wck0lO.HPBRNffzfPNqXRhAON.1E!/b&bo=QAHyAEAB8gABGT4!&rf=viewer_4" alt="Figure 3.11"></p>
<p>Users can train their own data sets through migration to better adapt to actual application scenarios. This paper adopts the network weight file (YOLOV4-416) trained by YOLO developers, which has included all the objects that need to be identified in this project, such as cars, pedestrians, traffic lights, stop signs, etc.</p>
<h3 id="3-4-3-Non-maximum-Suppression"><a href="#3-4-3-Non-maximum-Suppression" class="headerlink" title="3.4.3 Non-maximum Suppression"></a>3.4.3 Non-maximum Suppression</h3><p>Non-maximum Suppression, NMS algorithm for short. The idea is to search for local maximum value and suppress maximum value [20]. It is widely used in computer vision tasks, such as face recognition and target recognition. In target detection, there are many candidate boxes on the same target, which may overlap with each other. Therefore, non-maximum suppression is used to eliminate redundant boundary boxes and determine an optimal target boundary box. Figure 3.12 (left) shows the original result of target detection, and Figure 3.12 (right) shows the effect of using this algorithm.<br><img src="/Users/jordon/Desktop/Blog/source/_posts/Automatic-Driving-Experiment-Platform-Based-on-Machine-Vision.md" alt="Figure 3.12"></p>
<p>The algorithm first sorts the objects according to their confidence level (the confidence level is represented as a number between 0 and 1 in the upper left corner of each boundary box), and then adds the boundary boxes with the highest confidence level to a list. Then calculate the area of all boundary boxes and IoU (Intersection-over-Union) of the boundary box with the highest confidence and other candidate boxes, delete the boundary box whose IoU is greater than the threshold, and repeat this process until the list is empty [21].</p>
<h3 id="3-4-4-Acceleration-using-TensorRT"><a href="#3-4-4-Acceleration-using-TensorRT" class="headerlink" title="3.4.4 Acceleration using TensorRT"></a>3.4.4 Acceleration using TensorRT</h3><p>TensorRT is a high-performance inference optimizer for deep learning, which improves throughput and reduces delay during inference in the process of deep learning using GPU. It can be used for inference acceleration in super-large-scale data centers, embedded platforms and autonomous driving platforms. Applications deployed on GPU using TensorRT run 40 times faster than CPU platforms [22]. Figure 3.13 illustrates the difference between Training and Inference.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccDSsrNmWwB.0YRQU4bYeQKezxsRQZksCFzSYndrKfT3su1Zg67G103SusMFF2bRPaXqBgSGvQHrAGxTsmhY.88!/b&bo=FgXcAhYF3AIBGT4!&rf=viewer_4" alt="Figure 3.13"></p>
<p>Training includes two stages of forward propagation and backward propagation. For the training set, the model continuously modifies the weights of the neural network through error back propagation. Inference, on the other hand, only includes the stage of forward propagation, aiming at new data other than the training set, the key point of which is the prediction of new data [23].</p>
<p>TensorRT is an inference optimizer that optimizes trained models. When the network training is completed, the files generated by the training model will be stored in TensorRT, which no longer relies on the deep learning framework, and can be optimized for NVIDA GPU [23]. TensorRT optimization methods mainly include Layer &amp; Tensor Fusion and Weight &amp;Activation Precision Calibration.</p>
<p>In this project, due to the limited computational power of Jetson Nano, if directly running YOLO-V4, it can only reach 1 frame per second, but after using TensorRT acceleration, it can reach nearly 5FPS.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcc9Uku6JcuutV6jLqBZT3d*5psAMd.66SDhB3OcEevPzEQJC6DZuRrynZtTGlo8UzQ5GangpkAEQ1FkrGsktbbw!/b&bo=XgGsAV4BrAEBGT4!&rf=viewer_4" alt="Figure 3.14"></p>
<h3 id="3-4-5-Target-Filtering"><a href="#3-4-5-Target-Filtering" class="headerlink" title="3.4.5 Target Filtering"></a>3.4.5 Target Filtering</h3><p>This project uses the data sets included in the daily life of common objects, as the author using simulation field or laboratory at home, it will identify objects such as furniture, electrical appliances. Thus, it is essential to filter out unwanted objects outside roads, only keep objects such as cars, pedestrians, traffic lights. Figure 3.15 to Figure 3.17 shows the recognition of the objects under different scenarios.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcc9Uku6JcuutV6jLqBZT3d92oklZu*azB8FNMmpbHOQ.V3DU7ZF.Pr8lM2iFR2Zbf.cuH.3MwcsfMtCDU2QsBGM!/b&bo=cgR8AnIEfAIBGT4!&rf=viewer_4" alt="Figure 3.15"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcc9Uku6JcuutV6jLqBZT3d8xeUiVuQx4f3fVsDQn4arIPwz.fDhzoRY4b5J74lC2dys5QFzQacD4Ej*S0BLtN1A!/b&bo=oASYAqAEmAIBGT4!&rf=viewer_4" alt="Figure 3.16"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcc9Uku6JcuutV6jLqBZT3d.KLpudjQDzMxZbYfFsEcmjLvj0azK1PPJYs4n4UFZFEtswChPmR.o6tL0HWxN0A9E!/b&bo=kgSOApIEjgIBGT4!&rf=viewer_4" alt="Figure 3.17"></p>
<h3 id="3-4-6-Obstacle-avoidance-estimation-algorithm"><a href="#3-4-6-Obstacle-avoidance-estimation-algorithm" class="headerlink" title="3.4.6 Obstacle avoidance estimation algorithm"></a>3.4.6 Obstacle avoidance estimation algorithm</h3><p>Judging the distance of obstacle mainly depends on vision, with distance sensor as auxiliary. Specifically, it is to judge the spatial relationship between the boundary frame of the obstacle and the lane line where the car is. If the boundary frame of the obstacle is inside the two lane lines, a stop message will be sent to the car.</p>
<h3 id="3-4-7-Traffic-light-color-recognition-algorithm"><a href="#3-4-7-Traffic-light-color-recognition-algorithm" class="headerlink" title="3.4.7 Traffic light color recognition algorithm"></a>3.4.7 Traffic light color recognition algorithm</h3><p>In target detection, when a traffic light is identified, the traffic light boundary box is intercepted as the ROI of the algorithm. The captured ROI images are converted into the color model of HSV in OpenCV, in which the color parameters are hue (H), saturation (S) and value (V) [24]. The comparison of colors and parameters is shown in Table 3.2.</p>
<table>
<thead>
<tr>
<th></th>
<th>black</th>
<th>gray</th>
<th>white</th>
<th>red</th>
<th>orange</th>
<th>yellow</th>
<th>green</th>
<th>cyan</th>
<th>blue</th>
<th>pruple</th>
</tr>
</thead>
<tbody><tr>
<td>hmin</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>156</td>
<td>11</td>
<td>26</td>
<td>35</td>
<td>78</td>
<td>100</td>
<td>125</td>
</tr>
<tr>
<td>hmax</td>
<td>180</td>
<td>180</td>
<td>180</td>
<td>180</td>
<td>25</td>
<td>34</td>
<td>77</td>
<td>99</td>
<td>124</td>
<td>155</td>
</tr>
<tr>
<td>smin</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>43</td>
<td>43</td>
<td>43</td>
<td>43</td>
<td>43</td>
<td>43</td>
<td>43</td>
</tr>
<tr>
<td>smax</td>
<td>255</td>
<td>43</td>
<td>30</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
</tr>
<tr>
<td>vmin</td>
<td>0</td>
<td>46</td>
<td>221</td>
<td>46</td>
<td>46</td>
<td>46</td>
<td>46</td>
<td>46</td>
<td>46</td>
<td>46</td>
</tr>
<tr>
<td>vmax</td>
<td>46</td>
<td>220</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
<td>255</td>
</tr>
</tbody></table>
<p>The image is a binary image, so the white point (255) in the image represents the area where red appears, and the number of pixels of red and green is recorded respectively. If the Max value of red is the largest, the traffic light is judged to be red; similarly, if the Max value of green is the largest, the traffic light is judged to be green.</p>
<h2 id="3-5-Control-core-–-State-machine-Responsibility-chain"><a href="#3-5-Control-core-–-State-machine-Responsibility-chain" class="headerlink" title="3.5 Control core – State machine + Responsibility chain"></a>3.5 Control core – State machine + Responsibility chain</h2><h3 id="3-5-1-Driving-state-abstraction-and-typical-state-migration"><a href="#3-5-1-Driving-state-abstraction-and-typical-state-migration" class="headerlink" title="3.5.1 Driving state abstraction and typical state migration"></a>3.5.1 Driving state abstraction and typical state migration</h3><p>In this project, four states are abstracted according to road conditions in the real world, namely cruise state, waiting traffic light state, straight state and left/right turn state, as follows:<br>(1) Self-test state: this state does not belong to the abstraction from the real world, but is the self-test of the car before performing the task. Execute successively: left full rudder, right full rudder, forward, backward command, by timer control each task execution time, by encoder control forward and backward distance.</p>
<p>(2) Cruise state: “Self-check” performed, the program will switch to cruise state, the state corresponding to drivers driving on the ordinary roads in the real world, its main task is to keep the vehicle in the middle of the lane line, at the same time rely on visual detecting pavement obstacles ahead, identify the vehicles or pedestrians.  Making car following, braking, parking and other action to avoid accidents according to the distance. In this state, if detecting the traffic light, the car starts to slow down until it identifies the stop line at the intersection and switches to another state. This corresponds to the fact that in the real world, when a driver sees a traffic light in the driving process, it means that there is an intersection ahead. In order to ensure safety through the intersection, drivers should slow down for both red light and green light. When detecting the stop line at the intersection, it means that the car is going through an intersection. At this time, it need to judge according to the traffic light: when the red light is red, it should stop and wait before the stop line and enter the “waiting traffic light state”; When the light is green, it can go straight or turn left or right to enter the “straight state” or “left/right turn state”.</p>
<p>(3) Waiting traffic light state: this state is very simple, the car does not need to pay attention to lane lines, obstacles and other information, just “stare at” the traffic light, until the traffic light turns to green and it switches to “straight state” or “left/right turn state”.</p>
<p>(4) Straight state or left/right turn state: this state is actually the most difficult, because there is no lane line as a reference when crossing the intersection, so the driver needs to “blind drive”, but also pay attention to other vehicles and pedestrians. When the lane line is detected again, it means that the car has entered the road and cut back to the “cruise state” again.</p>
<p>The above four states can abstract all the basic road conditions encountered in the actual road. The four states have their own functions, and each state only needs to pay attention to the information that the state needs to pay attention to.</p>
<h3 id="3-5-2-Driving-control"><a href="#3-5-2-Driving-control" class="headerlink" title="3.5.2 Driving control"></a>3.5.2 Driving control</h3><p>The fuzzy control algorithm of steering angle is used in car steering. It is found through the test that the car camera can recognize two lane lines in the case of normal straight road. In this case, the deviation of the car from the lane line can be judged directly by judging the difference between the center point of the camera picture (which is fixed) and the lane line at the center point of the camera picture. At this time, the car steering angle can be established a simple function to adjust (turning angle = difference multiplied by a fixed coefficient); In the case of sharp bends, one lane line will exceed the visual field of the car camera, that is, only one lane line can be identified. In this case, the car needs a larger turning angle to stay in the middle of the lane. The linear function of lane line slope (turning angle = lane line slope multiplied by a fixed coefficient) is fitted.<br>angle(n)={(mid_angle +  (mid_diff *  turn_scale),&amp;n=2@<br>           mid_angle + lane_k*scale ,&amp;n=1)               (Formula 3-1)<br>In Formula (3-1) above, n is the number of lane lines. The speed of the car is maximum on the straight road. When it encounters a sharp bend, the speed of the car will decrease according to the turning angle. When there is a collision risk with obstacles such as cars and pedestrians in front of the car, the car will stop and wait until the obstacles move out of the safety distance.</p>
<h1 id="Chapter-4-experiment-and-debugging"><a href="#Chapter-4-experiment-and-debugging" class="headerlink" title="Chapter 4 experiment and debugging"></a>Chapter 4 experiment and debugging</h1><p>The picture of the car is shown in Figure 4.1 and 4.2.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccDSsrNmWwB.0YRQU4bYeQK3jKhguLTiMdl1BPei6dQ4jblxmVy7pYqNO..bkNzr.YQnhPgumsl9ZataSb3SdcQ!/b&bo=tAaAAuQGkgIBGXw!&rf=viewer_4" alt="Figure 4.1"></p>
<p>Table 4.1 lists the cost of the main hardware of the car in this project, and the total cost is 1600 yuan. </p>
<table>
<thead>
<tr>
<th>Hardware</th>
<th>Cost（yuan）</th>
</tr>
</thead>
<tbody><tr>
<td>On-board computer</td>
<td>800</td>
</tr>
<tr>
<td>Actuator control panel</td>
<td>98</td>
</tr>
<tr>
<td>Camera</td>
<td>150</td>
</tr>
<tr>
<td>Sensors</td>
<td>96</td>
</tr>
<tr>
<td>Battery</td>
<td>100</td>
</tr>
<tr>
<td>Car turf</td>
<td>223</td>
</tr>
<tr>
<td>Synchronous rectifier power supply</td>
<td>33</td>
</tr>
<tr>
<td>3D printing consumables</td>
<td>100</td>
</tr>
<tr>
<td>Total</td>
<td>1600</td>
</tr>
</tbody></table>
<h2 id="4-1-Debugging-of-dual-channel-inference-tracking-lane-line-detection-algorithm"><a href="#4-1-Debugging-of-dual-channel-inference-tracking-lane-line-detection-algorithm" class="headerlink" title="4.1 Debugging of dual channel inference tracking lane line detection algorithm"></a>4.1 Debugging of dual channel inference tracking lane line detection algorithm</h2><p>Figure 4.3 – Figure 4.7 shows gray scale transformation, Gaussian filtering, edge detection and masked area images.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccDSsrNmWwB.0YRQU4bYeQLbnksXvuA8tkHOVeWw6zlSehWJPWlxDhpWJCyk7kpUtwbETt2EPax9M3Xq5uM015I!/b&bo=pAH4AKQB.AABGT4!&rf=viewer_4" alt="Figure 4.3"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccDSsrNmWwB.0YRQU4bYeQLbnksXvuA8tkHOVeWw6zlSehWJPWlxDhpWJCyk7kpUtwbETt2EPax9M3Xq5uM015I!/b&bo=pAH4AKQB.AABGT4!&rf=viewer_4" alt="Figure 4.4"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcYl9tU*h3xDSuMuRgI1dmqJYD6zzaiMyKWVVsdy92zwcwHt4F1dTCtUSlX.wSXmr2arh2HjPK0HshRyAU1XsVxI!/b&bo=rAH.AKwB*gABGT4!&rf=viewer_4" alt="Figure 4.5"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcYl9tU*h3xDSuMuRgI1dmqJR.H*koMu0*ngbGiblQciPBJUCKXp7ZIt2M10z6*tpiZZd2zZFGBk7hiQs7NJQj8E!/b&bo=sAEAAbABAAEBGT4!&rf=viewer_4" alt="Figure 4.6"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcYl9tU*h3xDSuMuRgI1dmqIiDmVrh8USjsTnEk.asS3kuM.IijpjykX470ln6*Ivc6vz5leJRBwOMC0UiBTk380!/b&bo=xgEMAcYBDAEBGT4!&rf=viewer_4" alt="Figure 4.7"></p>
<p>In GaussianBlur, the width and height of the Gaussian kernel are both set to 5. In the Canny function, low threshold and high threshold are combined. It is found in the experiment that if only one threshold is used, only the more obvious edge in the image can be detected, and the edge will be discontinuous, so a smaller threshold is needed to connect the discontinuous edge. After debugging, the optimal effect is achieved when the low threshold and high threshold are set to 50 and 150 respectively in the environment of this project.</p>
<p>The white area in Figure 4.6 is the initial mask created by image size. The image within the masked area is shown in Figure 4.7. Update the next masked area with the identified current lane line.</p>
<p>Figure 4.8 shows the line obtained by quadratic fitting, and Figure 4.9 marks the line in the original image. Figure 4.10- Figure 4.12 shows the detection results in different scenarios. Intersection stop line (green horizontal line in Figure 4.13) has the same recognition algorithm principle as lane line.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcY6sLsRy*Hcm0v1f*QlyHagkN4wO6h3SWEHDarDbLSB8xY3yuLl8.Ghvofj*oL9otqtRVFOJ46A1qGzd6Bm25GI!/b&bo=zgESAc4BEgEBGT4!&rf=viewer_4" alt="Figure 4.8"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccDSsrNmWwB.0YRQU4bYeQKvO8HInzrHkQJpK16xcTvLfA4YhoF5i83xgAnhpPfsYAOJtkdBqxkWP4sXk0yIqLc!/b&bo=zAESAcwBEgEBGT4!&rf=viewer_4" alt="Figure 4.9"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcRUb5aROhQnlAWfYnD6rdnRfSFFLz.XRdgjHakPzfCQvS1RJQ4KXSMEPyPnjWgD7oE3kKnAlQG6ax.Cxo5TGuDs!/b&bo=tAMWArQDFgIBGT4!&rf=viewer_4" alt="Figure 4.10"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcRUb5aROhQnlAWfYnD6rdnQc9YGiTxVSgkWHrRRCECQRGqb8EsCbJ6l*3sRbNqB0RleocV47ORrroJVoKL9YN3w!/b&bo=tAMIArQDCAIBGT4!&rf=viewer_4" alt="Figure 4.11"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcRUb5aROhQnlAWfYnD6rdnTvQWyR9NxriheTEGKX9d63t5VsWOWROJApAPUreugft8a6811N0PPsOoAFQK5Aw0Q!/b&bo=tAMUArQDFAIBGT4!&rf=viewer_4" alt="Figure 4.12"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcRUb5aROhQnlAWfYnD6rdnSGDPSLHN1Uz9XkRDxKYtL*fdXC7DwS9g8Qo896plcGaTdMPNsMXPI9lWGoiuDYInU!/b&bo=uAMSArgDEgIBGT4!&rf=viewer_4" alt="Figure 4.13"></p>
<p>In Figure 4.13, the ROI area is the middle area of two lane line masks (red dotted box in Figure 4.13). In addition, the lane line is a horizontal line approximately parallel to the X axis, so the slope of the stop line is different from that of the lane line during filtering. Other steps are consistent with lane line detection.</p>
<h2 id="4-2-Automatic-driving-typical-scenario-debugging"><a href="#4-2-Automatic-driving-typical-scenario-debugging" class="headerlink" title="4.2 Automatic driving typical scenario debugging"></a>4.2 Automatic driving typical scenario debugging</h2><p>Typical scenarios of automatic driving include straight-line cruise, turning deceleration, intersection deceleration, traffic light intersection and vehicle avoidance, etc. The experimental results are given below respectively. It can be seen from Figure 4.14 that there is no obstacle in front of the car, so it runs at a fast speed. At the top of the image, it can be seen that the current speed (POWER) of the car is 70% (the maximum power of the car is 255, thus 178 indicates that the car runs at 70% power).<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcX9YkpwXMjycpUIMNBHqAumN.qdZxYlJ67g5JuwuFNhHvqQKVYDcp6GwNFdGT428Ts3oPhjXhpRNUk.6b9jF.nA!/b&bo=vgMeAr4DHgIBGT4!&rf=viewer_4" alt="Figure 4.14"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcX9YkpwXMjycpUIMNBHqAulLdVNvNROwGdmAC2bsKvem5COWoWReDVR1BQ*aXoc6KyvxAxCBWRvYE6jTXnvu**I!/b&bo=wAMaAsADGgIBGT4!&rf=viewer_4" alt="Figure 4.15"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcX9YkpwXMjycpUIMNBHqAulJK7vIwqNWKgHJH9OFXMEUpFf*ODauUATRU7useU8gc.SV6fsZvo*38nJRK0aHIQo!/b&bo=2AMoAtgDKAIBGT4!&rf=viewer_4" alt="Figure 4.16"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcX9YkpwXMjycpUIMNBHqAulpeeiAZcKgZ1W3FLIczwUkkXHPzLDxgSn5tocR5kc*fFSrT.0Wp3KL1zn9JEZSF3o!/b&bo=0gMqAtIDKgIBGT4!&rf=viewer_4" alt="Figure 4.17"></p>
<p>Figure 4.15 shows the car just entering a state of a sharp bend. In Figure 4.16 the car driving to the sharp bend, although it is unable to detect another lane, the car will be able to continue driving through algorithm according to the left lane line. In Figure 4.17 the car captures another lane line again, we can see red box in the image showing the speed and the change of car steering angle (Angle).<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcd7BAeMrDbikHMiI2rcHGustpKy6jdBEx6lBkWGGYBa3rLiK2G3wgsUldQ0s0YjQsblXxDh19UrDgFbLLd2KaBw!/b&bo=ygMgAsoDIAIBGT4!&rf=viewer_4" alt="Figure 4.18"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcd7BAeMrDbikHMiI2rcHGuvWdsymf1aRG3wafZJDiA1awuZHUbLAiJHEvBU3Y.9s*rZaopz.31nBe1ruBCoJXOE!/b&bo=1gMsAtYDLAIBGT4!&rf=viewer_4" alt="Figure 4.19"></p>
<p>Figure 4.18 shows the state of the car about to pass the intersection. It can be seen that the car has successfully recognized the traffic light and slowed down to 40% ready to pass the intersection. As shown in Figure 4.19, the car has successfully identified the intersection stop line (green horizontal line) and is ready to cross the intersection. Figure 4.20 shows the state of the car passing the intersection, and it can be seen that the road route of the car has been calculated by the opposite lane line. Figure 4.21 shows the car entering the opposite lane and successfully recapturing the lane line.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcd7BAeMrDbikHMiI2rcHGuuGX7gqZAObkQ4G5eW21rNLnBa4hfHN2AVc8bwPjFnWT4V2poGsRnULWIBgSwyhAtU!/b&bo=0gMmAtIDJgIBGT4!&rf=viewer_4" alt="Figure 4.10"><br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mcd7BAeMrDbikHMiI2rcHGuvG6p8T6697y7tJa1zikwfE6ZUwpF1AtAaXIj5dJsuQfHLW0v0sA4qQ6ZElCOiAd3o!/b&bo=2gMmAtoDJgIBGT4!&rf=viewer_4" alt="Figure 4.21"></p>
<p>Figure 4.22 shows the a vehicle avoidance scenario. At this point, the distance of the obstacle in front the car is less than the safety distance, and the car starts braking. It can be seen that the current speed of the car at the top of the image is 0%.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccx8YheB9cI3pJ0ZNHQfMqR5S.zk4EomWx8R8Z7f36JPBLRyOHvdqdAOr*Fz0NXGJCWZ9xnQ3ySAoS8YHIIDGR0!/b&bo=gASGAoAEhgIBGT4!&rf=viewer_4" alt="Figure 4.22"></p>
<h2 id="4-3-Remotely-Debugging-the-XServer"><a href="#4-3-Remotely-Debugging-the-XServer" class="headerlink" title="4.3 Remotely Debugging the XServer"></a>4.3 Remotely Debugging the XServer</h2><p>This project is deployed in Linux operating system, Linux graphical interface is not a necessary part of the system, it can run under the condition of no interface. When Linux is required to provide an interface, the system will establish one or several X Servers to interact with the window manager through the X protocol and generate interactive interfaces such as windows, status bars and buttons by applications independent of the system [26].</p>
<p>As SSH protocol can only transmit the command line, there is no graphical interface, but camera images transmission is needed in this project to debug. In order to more convenient to learn the car real-time recognition, the author use the X server remote debugging environment, which can transmit code on a laptop to the car, remote control of car and stop, and receive the feedback of the status of the car in real time.</p>
<p>Log file is a record file or collection of files that record system operation events, which can be divided into event log and message log. It plays an important role in processing historical data, tracking problems and understanding system activities [27].</p>
<p>During project debugging, it is often necessary to look at historical parameters to find program bugs. For example, if the car does not perform the operation correctly when passing the intersection, then the log file of the car can be checked to find the car’s final recognition, speed and state. After finding out the problem, the program can be modified. This just like the flight data recorder of an airplane.</p>
<h1 id="Chapter-5-summary-and-prospect"><a href="#Chapter-5-summary-and-prospect" class="headerlink" title="Chapter 5 summary and prospect"></a>Chapter 5 summary and prospect</h1><h2 id="5-1-summary"><a href="#5-1-summary" class="headerlink" title="5.1 summary"></a>5.1 summary</h2><p>In the automatic driving experiment platform, this paper has simulated most of the functions of auxiliary automatic driving, such as tracking lane lines, automatically braking when encountering obstacles, passing intersections with traffic lights, etc.</p>
<p>In the improved lane detection experiment, it is found that the detection rate is high, and there is little interference or loss. However, in the object detection experiment, it is found that the success rate of traffic light recognition is low, and the object detection is greatly affected by the environment and the contrast of the environment around the object, which has great challenge difficulty. In the debugging process in the laboratory, it was found that the overall success rate of image recognition would be affected by ambient light, and the debugging data in the dark environment at night could not be used in the daytime environment with sufficient light, so it was difficult to overcome the interference of light intensity for image recognition. The car in this project only uses an ultra-wide-angle camera, but it is difficult to identify the opposite lane line due to the lack of telephoto perspective when passing the intersection, thus increasing the difficulty of this scheme.</p>
<p>The main purpose of the project is to build an autonomous driving platform to achieve low cost, small volume and high integration, which is very suitable for the promotion of students. It is an experimental platform for sustainable research, and it is easy to iterate, practice and verify new algorithms and new ideas in the later stage.</p>
<h2 id="5-2-prospect"><a href="#5-2-prospect" class="headerlink" title="5.2 prospect"></a>5.2 prospect</h2><p>(1) The current lane line detection algorithm cannot draw curved lane lines. Instead, straight lines are used to determine the curved lane lines by calculating the slope of the line. The author also tried to use machine learning method to detect lane lines, so that the range of lane lines could be drawn more accurately. However, due to the limitations of hardware conditions, the computing power of Jetson Nano could not process both lane line model and YOLO model at the same time, so the author planned to upgrade the hardware to improve the computing power.</p>
<p>(2) As mentioned above, current YOLO-V4 uses officially trained data sets, but this model can identify many objects in daily life, such as furniture, electrical appliances, animals, etc. In this project, only objects encountered on the road need to be identified, thus it wastes a lot of resources and computing power. In the later stage, the author plans to train his own target detection data set, including the contents that cannot be identified in the existing data set, such as various traffic signs, roadblocks, landmarks, etc.</p>
<p>(3) In various driverless cars with assistance at present, each car has cameras with multiple perspectives to detect the situation around the vehicle. For example, each Tesla car has 8 cameras. In the later stage, the author plans to add three more cameras (two on the side and one on the rear), so as to cope with the complicated road conditions at the intersection and achieve overtaking.</p>
<p>(4) In the later stage, the author intends to imitate Tesla and make a third-view UI similar to Figure 5.1 (right), which can more comprehensively show the situation around the car.<br><img src="http://m.qpic.cn/psc?/V52O6T4v1BaKX82y9mu62U2PpX2MjRTM/45NBuzDIW489QBoVep5mccx8YheB9cI3pJ0ZNHQfMqT5Tp56uKMiIvSu*zkes8z4WNIusx1np9Cw0rlkZyenX0yVdWhr92NvebLljBz1VVM!/b&bo=kgdoApIHaAIBGT4!&rf=viewer_4" alt="Figure 5.1"></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[1]    任燕宏. 关于无人驾驶汽车障碍物信息识别与处理系统的研究[D].兰州理工大学,2011.<br>[2]    《中国公路学报》编辑部.中国汽车工程学术研究综述[J].中国公路学报,2017,30(06):1-197.<br>[3]    托斯. 人工智能时代[M].人民邮电出版社, 2017.<br>[4]    中国电子信息产业发展研究院. 数字丝绸之路[M].人民邮电出版社, 2017.书后面只加年份<br>[5]    刘勇. 基于深度卷积神经网络的空中目标识别算法的研究[D].沈阳理工大学,2020.<br>[6]    Chollet, Francois. Deep Learning with Python. Manning[J], 2017:256.<br>[7]    闫施违. 自动驾驶系统测试用例生成技术的实证研究[D].哈尔滨工业大学,2019.<br>[8]    刘永胜. 基于深度神经网络的弱监督学习方法在图像领域的研究[D].电子科技大学,2020.<br>[9]    莫会成,闵琳.现代高性能永磁交流伺服系统综述——传感装置与技术篇[J].电工技术学报,2015,30(06):10-21.<br>[10] 张兴. PWM整流器及其控制策略的研究[D].合肥工业大学,2003.<br>[11] 弘毅. 舵机详解[EB/OL], <a target="_blank" rel="noopener" href="https://www.geek-workshop.com/thread-70-1-1.html">https://www.geek-workshop.com/thread-70-1-1.html</a>, 2011.<br>[12]    王子杨. 基于双目视觉的三维目标检测与跟踪算法研究[D].华南理工大学,2020.<br>[13]    王林章,何剑涛,韩志雄,卜磊,赵建华,李宣东.设计模式指导的软件分簇方法[J].中国科学:信息科学,2014,44(01):37-53.<br>[14]    张宇,庄晋林.面向对象设计原则和设计模式的应用[J].华北水利水电学院学报,2007(01):63-65.<br>[15]    伍健. 基于PDE和全变分滤波方法的研究及在多种噪声中的应用[D].天津大学,2012.<br>[16]    高飞. MATLAB图像处理375例[M].人民邮电出版社:, 201510.504.<br>[17]    丁威. 智能辅助驾驶场景典型目标识别技术研究[D].电子科技大学,2020.<br>[18]    龚声蓉,刘纯平,季怡. 复杂场景下图像与视频分析[M].人民邮电出版社, 2013.<br>[19]    Alexey Bochkovskiy. YOLOv4: Optimal Speed and Accuracy of Object Detection[M]. arXiv:2004.10934v1<br>[20]     陈金辉,叶西宁.行人检测中非极大值抑制算法的改进[J].华东理工大学学报(自然科学版),2015,41(03):371-378.<br>[21]    赵春晖,周瑶.基于改进Faster R-CNN算法的舰船目标检测与识别[J].沈阳大学学报(自然科学版),2018,30(05):366-371+380.<br>[22]    周立君,刘宇,白璐,刘飞,王亚伟.使用TensorRT进行深度学习推理[J].应用光学,2020,41(02):337-341.<br>[23]    陈凯. 深度学习模型的高效训练算法研究[D].中国科学技术大学,2016.<br>[24]    李琳,张涛.结合改进聚合通道特征和灰度共生矩阵的俯视行人检测算法[J].计算机应用,2018,38(12):3367-3371+3398.<br>[25]    【OpenCV学习笔记】之霍夫变换（Hough Transform）[EB/OL]，<a target="_blank" rel="noopener" href="https://blog.csdn.net/zhu_hongji/article/details/81632611">https://blog.csdn.net/zhu_hongji/article/details/81632611</a><br>[26]    刘金凤,赵鹏舒,祝虹媛. 计算机软件基础[M].哈尔滨工业大学出版社, 2012.<br>[27]    李学龙,龚海刚.大数据系统综述[J].中国科学:信息科学,2015,45(01):1-44.<br>[28]    黄磊. 面向对象开发参考手册[M].人民邮电出版社:, 201401.272.<br>[29]    万莉莉.Moore型和Mealy型有限状态机的VHDL设计[J].科技信息(学术研究),2007(31):220-222.<br>[30]    吝春妮, 杨潇. 软件设计之责任链模式[J]. 科学之友(B版), 2009(01):145-147+150.<br>[31]    周丽,焦程波,兰巨龙.LINUX系统下多线程与多进程性能分析[J].微计算机信息,2005(17):118-120+149.<br>[32]    Gamma E, Helm R, Johnson R E, et al. Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley, Reading, MA. Addison-Wesley Longman Publishing Co.Inc.1994.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jordon_Xu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2021/10/02/Automatic-Driving-Experiment-Platform-Based-on-Machine-Vision/">http://example.com/2021/10/02/Automatic-Driving-Experiment-Platform-Based-on-Machine-Vision/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/cover_2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/01/Autonomous-driving-experimental-prototype/"><img class="next-cover" src="/img/cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Autonomous driving experimental prototype (Video)</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/1.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Jordon_Xu</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">2</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">3</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">1</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Jordon-Xu" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:jordon.zhehan@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Abstract"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter-1-Introduction"><span class="toc-number">2.</span> <span class="toc-text">Chapter 1 Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Project-Background"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 Project Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Development-status-of-autonomous-driving-technology"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 Development status of autonomous driving technology</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-Breakthroughs-and-accumulation-of-artificial-intelligence"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 Breakthroughs and accumulation of artificial intelligence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-Objective-and-significance-of-the-project"><span class="toc-number">2.4.</span> <span class="toc-text">1.4 Objective and significance of the project</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter-2-Automatic-driving-experiment-platform-construction"><span class="toc-number">3.</span> <span class="toc-text">Chapter 2 Automatic driving experiment platform construction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Modular-design-of-model-car"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 Modular design of model car</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1-Trolley-chassis"><span class="toc-number">3.1.1.</span> <span class="toc-text">2.1.1 Trolley chassis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2-Vehicle-mounted-device-connection-module"><span class="toc-number">3.1.2.</span> <span class="toc-text">2.1.2 Vehicle-mounted device connection module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-3-On-board-computer"><span class="toc-number">3.1.3.</span> <span class="toc-text">2.1.3 On-board computer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-4-Motor-driver-board"><span class="toc-number">3.1.4.</span> <span class="toc-text">2.1.4 Motor driver board</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-5-camera"><span class="toc-number">3.1.5.</span> <span class="toc-text">2.1.5 camera</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-6-Range-sensor"><span class="toc-number">3.1.6.</span> <span class="toc-text">2.1.6 Range sensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-7-Battery-and-voltage-regulator-power-supply"><span class="toc-number">3.1.7.</span> <span class="toc-text">2.1.7 Battery and voltage regulator power supply</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Design-of-simulated-experimental-site"><span class="toc-number">3.2.</span> <span class="toc-text">2.2. Design of simulated experimental site</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1-Design-and-splicing-of-experimental-roads"><span class="toc-number">3.2.1.</span> <span class="toc-text">2.2.1 Design and splicing of experimental roads</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2-Simulate-traffic-signs-vehicles-and-pedestrians"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.2.2 Simulate traffic signs, vehicles and pedestrians</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Software-Environment"><span class="toc-number">3.3.</span> <span class="toc-text">3.1 Software Environment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Event-driven-real-time-system-framework"><span class="toc-number">3.4.</span> <span class="toc-text">3.2 Event-driven real-time system framework</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-Practice-the-idea-of-object-oriented-programming-through-self-built-framework"><span class="toc-number">3.4.1.</span> <span class="toc-text">3.2.1 Practice the idea of object-oriented programming through self-built framework</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Multithreading-structure"><span class="toc-number">3.4.2.</span> <span class="toc-text">3.2.2 Multithreading structure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-Framework-design"><span class="toc-number">3.4.3.</span> <span class="toc-text">3.2.3 Framework design</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Core-algorithm-%E2%80%93-Dual-channel-inference-tracking-lane-line-detection-algorithm"><span class="toc-number">3.5.</span> <span class="toc-text">3.3 Core algorithm – Dual channel inference tracking lane line detection algorithm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-Optimized-dual-channel-inference-tracking-ROI-algorithm"><span class="toc-number">3.5.1.</span> <span class="toc-text">3.3.1 Optimized dual channel inference tracking ROI algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-Extraction-and-fitting-of-lane-lines"><span class="toc-number">3.5.2.</span> <span class="toc-text">3.3.3 Extraction and fitting of lane lines</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-Core-algorithm-%E2%80%93-YOLO-target-detection"><span class="toc-number">3.6.</span> <span class="toc-text">3.4 Core algorithm – YOLO target detection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-YOLO-V4-network-structure"><span class="toc-number">3.6.1.</span> <span class="toc-text">3.4.1 YOLO-V4 network structure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-YOLO-network-training"><span class="toc-number">3.6.2.</span> <span class="toc-text">3.4.2 YOLO network training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-Non-maximum-Suppression"><span class="toc-number">3.6.3.</span> <span class="toc-text">3.4.3 Non-maximum Suppression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-4-Acceleration-using-TensorRT"><span class="toc-number">3.6.4.</span> <span class="toc-text">3.4.4 Acceleration using TensorRT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-5-Target-Filtering"><span class="toc-number">3.6.5.</span> <span class="toc-text">3.4.5 Target Filtering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-6-Obstacle-avoidance-estimation-algorithm"><span class="toc-number">3.6.6.</span> <span class="toc-text">3.4.6 Obstacle avoidance estimation algorithm</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-7-Traffic-light-color-recognition-algorithm"><span class="toc-number">3.6.7.</span> <span class="toc-text">3.4.7 Traffic light color recognition algorithm</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-Control-core-%E2%80%93-State-machine-Responsibility-chain"><span class="toc-number">3.7.</span> <span class="toc-text">3.5 Control core – State machine + Responsibility chain</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-1-Driving-state-abstraction-and-typical-state-migration"><span class="toc-number">3.7.1.</span> <span class="toc-text">3.5.1 Driving state abstraction and typical state migration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-2-Driving-control"><span class="toc-number">3.7.2.</span> <span class="toc-text">3.5.2 Driving control</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter-4-experiment-and-debugging"><span class="toc-number">4.</span> <span class="toc-text">Chapter 4 experiment and debugging</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Debugging-of-dual-channel-inference-tracking-lane-line-detection-algorithm"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 Debugging of dual channel inference tracking lane line detection algorithm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-Automatic-driving-typical-scenario-debugging"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 Automatic driving typical scenario debugging</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Remotely-Debugging-the-XServer"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 Remotely Debugging the XServer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Chapter-5-summary-and-prospect"><span class="toc-number">5.</span> <span class="toc-text">Chapter 5 summary and prospect</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-summary"><span class="toc-number">5.1.</span> <span class="toc-text">5.1 summary</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-prospect"><span class="toc-number">5.2.</span> <span class="toc-text">5.2 prospect</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-number">6.</span> <span class="toc-text">Reference</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/02/Automatic-Driving-Experiment-Platform-Based-on-Machine-Vision/" title="Automatic Driving Experiment Platform Based on Machine Vision"><img src="/img/cover_2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Automatic Driving Experiment Platform Based on Machine Vision"/></a><div class="content"><a class="title" href="/2021/10/02/Automatic-Driving-Experiment-Platform-Based-on-Machine-Vision/" title="Automatic Driving Experiment Platform Based on Machine Vision">Automatic Driving Experiment Platform Based on Machine Vision</a><time datetime="2021-10-02T03:10:12.000Z" title="Created 2021-10-02 11:10:12">2021-10-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/01/Autonomous-driving-experimental-prototype/" title="Autonomous driving experimental prototype (Video)"><img src="/img/cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Autonomous driving experimental prototype (Video)"/></a><div class="content"><a class="title" href="/2021/10/01/Autonomous-driving-experimental-prototype/" title="Autonomous driving experimental prototype (Video)">Autonomous driving experimental prototype (Video)</a><time datetime="2021-10-01T15:33:50.000Z" title="Created 2021-10-01 23:33:50">2021-10-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Jordon_Xu</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="Increase font size"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="Decrease font size"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>